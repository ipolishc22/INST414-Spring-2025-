{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "fn = input(\"Enter the file name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, \"r\") as in_file:\n",
    "    input_data = in_file.read()\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = tokenizer.tokenize(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "token_counter = dict()\n",
    "for token in token_list:\n",
    "    if token in string.punctuation: \n",
    "        continue \n",
    "    token=token.lower()\n",
    "\n",
    "    if token not in token_counter:\n",
    "        token_counter[token] = 0\n",
    "    token_counter[token] = token_counter[token]+1\n",
    "print(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens = sorted(token_counter, key=token_counter.get, reverse=True)\n",
    "for t in sorted_tokens[:10]:\n",
    "    print(t, token_counter[t])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
